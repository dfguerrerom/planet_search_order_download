{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.functions import *\n",
    "from parameters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to use your own api keys and parameters, copy paste the `parameters.py.dist` file in the same folder and remove the `.dist` extention. You can then replace the string with your own keys. only the .dist will be pushed to the dist git rep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Search items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bounding box from centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a projected EPSG for the centroids file\n",
    "# EPSG:21148 is for Indonesia.\n",
    "samples_gdf = read_from_centroids(projected_epsg='EPSG:21148', buffer=350, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the first geometry\n",
    "# print(samples_gdf.iloc[0].geometry.area)\n",
    "# samples_gdf.iloc[0].geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = api.ClientV1(api_key=PLANET_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_type_score\n",
    "item_type_score = {\n",
    "    'PSScene4Band':10, \n",
    "    'PSScene3Band':8, \n",
    "    'PSOrthoTile':8,\n",
    "    'REOrthoTile':0,\n",
    "    'SkySatScene':0,\n",
    "}\n",
    "\n",
    "# season score\n",
    "months_score = {\n",
    "    1: 0, 7:8,\n",
    "    2: 0, 8:10,\n",
    "    3: 0, 9:10,\n",
    "    4: 7, 10:8,\n",
    "    5: 7, 11:0,\n",
    "    6: 7, 12:0,\n",
    "}\n",
    "\n",
    "# cloud_score\n",
    "\n",
    "def cloud_score(cloud_cover):\n",
    "    \"\"\" Define the cloud cover threshold and score\n",
    "    \n",
    "    1 = 1%\n",
    "    \n",
    "    \"\"\"\n",
    "    cloud_cover = cloud_cover*100\n",
    "    \n",
    "    if cloud_cover == 0:\n",
    "        return 10\n",
    "    elif cloud_cover <= 5 and cloud_cover > 0:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Covered area\n",
    "\n",
    "def cover_score(covered_area):\n",
    "    \"\"\"Define the cover area threshold and score\n",
    "    \"\"\"\n",
    "    covered_area = covered_area*100\n",
    "    \n",
    "    if covered_area >= 99:\n",
    "        return 10\n",
    "    \n",
    "    elif covered_area >= 95:\n",
    "        return 5\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION 1.2 Get items for all plots and store into a big df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection method\n",
    "The loop will search all the images between the given start-end date, and the minimum cloud coverage.<br>\n",
    "After that it will calculate the sample covered area with the image item footprint and then will remove items which are under the given threshold.<br>\n",
    "The next step is rank the items by the selected parameters <br>\n",
    "#### Temporal selection\n",
    "The user has to select the desired time span for get the images: 1 per year, 1 per month, or one every x images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test data for the filter\n",
    "start_date = datetime.datetime(2017, 1, 1)\n",
    "stop_date = datetime.datetime(2020, 12, 31)\n",
    "cloud_cover_lte = 0.10\n",
    "minimum_covered_area = 90 # included\n",
    "\n",
    "# If by_month is True, one image per month will be chosen, otherwise one per year.\n",
    "# By default it will process only one image per year\n",
    "\n",
    "by_month = False\n",
    "by_every = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over all plots in parellel\n",
    "Loop over all plots and get the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiprocess(index, row, srch_log_file, by_month=False, by_every=0, skip_items=None):\n",
    "    \n",
    "    aoi_geometry = json.loads(dumps(row.geometry))\n",
    "    sample_id = row.name\n",
    "    \n",
    "    if by_every:\n",
    "        pickle_df_name = os.path.join(OUT_PIKL_PATH, str(sample_id)+'_every.p')\n",
    "    elif by_month:\n",
    "        pickle_df_name = os.path.join(OUT_PIKL_PATH, str(sample_id)+'_month.p')\n",
    "    else:\n",
    "        pickle_df_name = os.path.join(OUT_PIKL_PATH, str(sample_id)+'_year.p')\n",
    "        \n",
    "    if not os.path.exists(pickle_df_name):\n",
    "        request = build_request(aoi_geometry, start_date, stop_date, cloud_cover_lte)\n",
    "\n",
    "        try:\n",
    "            print(f'Starting {sample_id}')\n",
    "            items = get_items(sample_id, request, client)\n",
    "            # Transform items into a pandas dataframe with useful columns\n",
    "            metadata_df = get_dataframe(items)\n",
    "            \n",
    "            \n",
    "            # Skip items with errors\n",
    "            if skip_items:\n",
    "                skip_items = [x[1] for x in skip_items]\n",
    "                metadata_df = metadata_df[~metadata_df.id.isin(skip_items)]\n",
    "            \n",
    "            # Mutate metadata_df and add the percentage of cover area\n",
    "            add_cover_area(metadata_df, samples_gdf)\n",
    "\n",
    "            # Remove items that are under the minimum_covered_area threshold\n",
    "            metadata_df = metadata_df[metadata_df.cover_perc >= (minimum_covered_area/100)]\n",
    "\n",
    "            # Create a score for each item\n",
    "            scored_items = score_items(metadata_df, item_type_score, months_score, cloud_score, cover_score)\n",
    "            \n",
    "            if by_every:\n",
    "                # Filter scored_items and get one item every x items\n",
    "                selected_items = get_one_item_every_x(scored_items, every=by_every)\n",
    "            \n",
    "            elif by_month:\n",
    "                # Filter scored_items and get only one per month\n",
    "                selected_items = get_one_item_per_month(scored_items)\n",
    "            else:\n",
    "                # Filter scored_items and get only one per year\n",
    "                selected_items = get_one_item_per_year(scored_items)\n",
    "            \n",
    "            # Save into a pickled file\n",
    "            print(f'Final lenght: {len(selected_items)}')\n",
    "            selected_items.to_pickle(pickle_df_name)\n",
    "            \n",
    "            print(f'{sample_id} pickled.')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'there was an error with the sample {sample_id}, please check the log files.')\n",
    "            with open(srch_log_file, 'a') as lf:\n",
    "                lf.write(f'\"{sample_id}\":{e}\\n')\n",
    "\n",
    "    else:\n",
    "        print(f'Search for {sample_id} already saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip error items from logs\n",
    "Uncomment the next cell if you have a log file with \"no access to assets\" elements, so the process will skip them.\n",
    "\n",
    "<br> If you are using this option, please delete the previous searches pickled files from the failed samples (search failed samples with the commands in step 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_items = None\n",
    "# skip_items = get_no_access_assets_from_log('logs/order_logs_20200925_14_56.txt')\n",
    "# len(skip_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Create a log file\n",
    "    now = datetime.datetime.now()\n",
    "    formated_now = now.strftime('%Y%m%d_%H_%M')\n",
    "    srch_log_file = os.path.join(LOG_PATH, f'search_logs_{formated_now}.txt')\n",
    "    \n",
    "    # Set the number of parallel processes\n",
    "    pool = multiprocessing.Pool(10)\n",
    "    \n",
    "    for index, row in samples_gdf.iterrows():\n",
    "        pool.apply_async(run_multiprocess, args=(index, row, srch_log_file, by_month, by_every, skip_items))\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the pickled files, merge and store them in a big df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickled_files = glob.glob(os.path.join(OUT_PIKL_PATH,'*every.p'))\n",
    "len(pickled_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([pd.read_pickle(pkl) for pkl in pickled_files])\n",
    "# all_df.to_pickle('searches/NAME_OF_PICKLE_FILE.p')\n",
    "# all_df = pd.read_pickle('searches/NAME_OF_PICKLE_FILE.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'there are {len(all_df)} items in the current df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL STEP) Add clear percent metadata (udm2) to images after 2018\n",
    "This step is intended to be used when creating a dense time series (using by_every or by_month option), not for one image per year.\n",
    "\n",
    "To more info about the udm2 metadata refer to: https://developers.planet.com/docs/data/udm-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following lines to extract the clear_percent and clear_confidence_percent metadata from \n",
    "# items which have this data.\n",
    "\n",
    "all_df['clear_percent'] = None\n",
    "all_df['clear_confidence_percent'] = None\n",
    "pbar = tqdm(total=len(all_df))\n",
    "for idx, row in all_df.iterrows():\n",
    "    if 'clear_percent' in list(row.metadata['properties'].keys()):\n",
    "        all_df.at[idx, 'clear_percent'] = row.metadata['properties']['clear_percent']\n",
    "        all_df.at[idx, 'clear_confidence_percent'] = row.metadata['properties']['clear_confidence_percent']\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After adding this metadata we can filter the images according to our specific needs\n",
    "min_clear_area = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to select the items which are above the min_clear_area, and remove those whose are below\n",
    "items_with_clear_percent = all_df[all_df.clear_percent >= min_clear_area]\n",
    "print(f'There are {len(items_with_clear_percent)} items with more than {min_clear_area} clear percentage')\n",
    "\n",
    "# And add those items which do not have the clear_percent metadata\n",
    "items_without_clear_percent = all_df[all_df.clear_percent.isnull()]\n",
    "print(f'There are {len(items_without_clear_percent)} items whose do not have clear_percentage metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can merge them in the all_df\n",
    "all_df = pd.concat([items_with_clear_percent, items_without_clear_percent])\n",
    "print(f'There is a total of {len(all_df)} in the current search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Order assets\n",
    "### Create json request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_bundles = {\n",
    "\n",
    "    # Is not possible to ask for analytic_dn in PSScene3Band, so the next option is visual\n",
    "    # for more info go to https://developers.planet.com/docs/orders/product-bundles-reference/\n",
    "    'PSScene3Band': \"analytic,visual\",\n",
    "    'PSScene4Band': \"analytic,analytic_udm2,analytic_sr\",\n",
    "    'PSOrthoTile': \"analytic,analytic_5b_udm2,analytic_5b,analytic_udm2,visual\",\n",
    "    'REOrthoTile': \"analytic,visual\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the order we need a dataframe with filtered items,\n",
    "# and a samples_gdf with sample_id and geometry to clip each item.\n",
    "\n",
    "# Set a prefix for the order name:\n",
    "prefix = ''\n",
    "partial = False\n",
    "\n",
    "# Build an order for each sample and store in a orders_list\n",
    "orders = []\n",
    "samples_ids = list(all_df.sample_id.unique())\n",
    "for idx, row in samples_gdf.iterrows():\n",
    "    if idx in samples_ids:\n",
    "        order = build_order_from_metadata(all_df, idx, row, products_bundles, prefix, partial)\n",
    "        orders.append(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request order\n",
    "<font color='red'>The following lines will start the order in the planet server, once the order is placed and running, there is no way to stop it.</font>\n",
    "\n",
    "NOTE: The following loop will skip the samples that have already been downloaded, however it's based on the existing_orders request, and we are not sure how long the requests will remain in the planet server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can use the pages parameter to limit the number of pages to be consulted\n",
    "# Every page has 20 elements, so is highly recommend to let it as None to avoid duplicate orders.\n",
    "# Limit only when you are sure that doesn't have ordered a sample before.\n",
    "pages = None \n",
    "\n",
    "# Request the existing orders and store their sample_id (name)\n",
    "current_server_orders = get_existing_orders(client, pages=pages)\n",
    "ordered_sample_names = [order['name'] for order in current_server_orders]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "formated_now = now.strftime('%Y%m%d_%H_%M')\n",
    "ordr_log_file = os.path.join(LOG_PATH, f'order_logs_{formated_now}.txt')\n",
    "\n",
    "orders_info = []\n",
    "pbar = tqdm(total=len(orders))\n",
    "for new_order in orders:\n",
    "\n",
    "    # Make sure that the sample is not already downloaded\n",
    "    sample_name = new_order['name']\n",
    "    if sample_name not in ordered_sample_names:\n",
    "        \n",
    "        try:\n",
    "            # The following line will create the order in the server\n",
    "            @backoff.on_exception(backoff.expo,(planet.api.exceptions.OverQuota,\n",
    "                                               planet.api.exceptions.TooManyRequests),max_time=360)\n",
    "            def place_order():\n",
    "                response = client.create_order(new_order).get()\n",
    "                return response\n",
    "            \n",
    "            order_info = place_order()\n",
    "            orders_info.append(order_info)\n",
    "            \n",
    "            order_id = order_info['id']\n",
    "            sample_name = order_info['name']\n",
    "            \n",
    "            print(f'order {order_id} with {sample_name} has been placed.')\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open(ordr_log_file, 'a') as lf:\n",
    "                print(f'there was an error with the sample {sample_name}, please check the log files.')\n",
    "                lf.write(f'Sample {sample_name}:{e}\\n')\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Additional commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_samples = [x[0] for x in get_no_access_assets_from_log('logs/order_logs_20200925_14_56.txt')]\n",
    "len(failed_samples)\n",
    "print(failed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "failed_items_ids = [x[1] for x in get_no_access_assets_from_log('logs/order_logs_20200925_14_56.txt')]\n",
    "failed_items_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
