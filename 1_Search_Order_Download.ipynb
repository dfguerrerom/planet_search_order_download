{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.config({\n",
       "    paths: {\n",
       "        datatables: 'https://cdn.datatables.net/1.10.19/js/jquery.dataTables.min',\n",
       "    }\n",
       "});\n",
       "\n",
       "$('head').append('<link rel=\"stylesheet\" type=\"text/css\" \\\n",
       "                href = \"https://cdn.datatables.net/1.10.19/css/jquery.dataTables.min.css\" > ');\n",
       "\n",
       "$('head').append('<style> table td { text-overflow: ellipsis; overflow: hidden; } </style>');\n",
       "\n",
       "$('head').append(`<script>\n",
       "function eval_functions(map_or_text) {\n",
       "    if (typeof map_or_text === \"string\") {\n",
       "        if (map_or_text.startsWith(\"function\")) {\n",
       "            try {\n",
       "                // Note: parenthesis are required around the whole expression for eval to return a value!\n",
       "                // See https://stackoverflow.com/a/7399078/911298.\n",
       "                //\n",
       "                // eval(\"local_fun = \" + map_or_text) would fail because local_fun is not declared\n",
       "                // (using var, let or const would work, but it would only be declared in the local scope\n",
       "                // and therefore the value could not be retrieved).\n",
       "                const func = eval(\"(\" + map_or_text + \")\");\n",
       "                if (typeof func !== \"function\") {\n",
       "                    // Note: backquotes are super convenient!\n",
       "                    // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals\n",
       "                    console.error(\"Evaluated expression \" + map_or_text + \" is not a function (type is \" + typeof func + \")\");\n",
       "                    return map_or_text;\n",
       "                }\n",
       "                // Return the function\n",
       "                return func;\n",
       "            } catch (e) {\n",
       "                // Make sure to print the error with a second argument to console.error().\n",
       "                console.error(\"itables was not able to parse \" + map_or_text, e);\n",
       "            }\n",
       "        }\n",
       "    } else if (typeof map_or_text === \"object\") {\n",
       "        if (map_or_text instanceof Array) {\n",
       "            // Note: \"var\" is now superseded by \"let\" and \"const\".\n",
       "            // https://medium.com/javascript-scene/javascript-es6-var-let-or-const-ba58b8dcde75\n",
       "            const result = [];\n",
       "            // Note: \"for of\" is the best way to iterate through an iterable.\n",
       "            // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for...of\n",
       "            for (const item of map_or_text) {\n",
       "                result.push(eval_functions(item));\n",
       "            }\n",
       "            return result;\n",
       "\n",
       "            // Alternatively, more functional approach in one line:\n",
       "            // return map_or_text.map(eval_functions);\n",
       "        } else {\n",
       "            const result = {};\n",
       "            // Object.keys() is safer than \"for in\" because otherwise you might have keys\n",
       "            // that aren't defined in the object itself.\n",
       "            //\n",
       "            // See https://stackoverflow.com/a/684692/911298.\n",
       "            for (const item of Object.keys(map_or_text)) {\n",
       "                result[item] = eval_functions(map_or_text[item]);\n",
       "            }\n",
       "            return result;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    return map_or_text;\n",
       "}\n",
       "</` + 'script>');"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts.functions import *\n",
    "from parameters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to use your own api keys and parameters, copy paste the `parameters.py.dist` file in the same folder and remove the `.dist` extention. You can then replace the string with your own keys. only the .dist will be pushed to the dist git rep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Search items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bounding box from centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-efaed87ab8d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msamples_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msamples_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EPSG:21148'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msamples_gdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_gdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0msamples_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPSG:4326\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msamples_gdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_gdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvelope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'buffer' is not defined"
     ]
    }
   ],
   "source": [
    "#create a geoDataFrame object from a .txt file\n",
    "if os.path.isfile(FILENAME):\n",
    "\n",
    "    df = pd.read_csv(FILENAME, sep=';')\n",
    "\n",
    "    #filter only the `nb_rows` first rows\n",
    "    nb_rows = 1#len(df)\n",
    "    filter_df  = df[df.index.isin(range(nb_rows))]\n",
    "    df = filter_df\n",
    "\n",
    "    #create the geodataframe \n",
    "    pts = [Point(df.loc[i][FILE_LNG], df.loc[i][FILE_LAT]) for i in range(len(df))]\n",
    "    samples_gdf = gpd.GeoDataFrame(data={'geometry': pts}, index=df[FILE_ID], crs=\"EPSG:4326\")\n",
    "    samples_gdf.index.names = ['id']\n",
    "    samples_gdf = samples_gdf.to_crs('EPSG:21148')\n",
    "    samples_gdf['geometry'] = samples_gdf['geometry'].buffer(buffer)\n",
    "    samples_gdf = samples_gdf.to_crs(\"EPSG:4326\")\n",
    "    samples_gdf['geometry'] = samples_gdf['geometry'].envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gdf = read_from_centroids(projected_epsg='EPSG:21148', buffer=350, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/geopandas/geodataframe.py:852: UserWarning: Geometry column does not contain geometry.\n",
      "  warnings.warn(\"Geometry column does not contain geometry.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><table id=\"517351af-f311-4e61-bfef-40efdd3e63d8\" class=\"display nowrap\"><thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead></table>\n",
       "<script type=\"text/javascript\">\n",
       "require([\"datatables\"], function (datatables) {\n",
       "    $(document).ready(function () {        \n",
       "        var dt_args = {\"columnDefs\": [{\"width\": \"70px\", \"targets\": \"_all\"}], \"paging\": false, \"data\": [[\"BRG_620801_02\", \"POLYGON ((111.1633130010653 -2.892815023311756, 111.169574970425 -2.892815023311756, 111.169574970425 -2.88651897473922, 111.1633130010653 -2.88651897473922, 111.1633130010653 -2.892815023311756))\"]]};\n",
       "        dt_args = eval_functions(dt_args);\n",
       "        table = $('#517351af-f311-4e61-bfef-40efdd3e63d8').DataTable(dt_args);\n",
       "    });\n",
       "})\n",
       "</script>\n",
       "</div>\n"
      ],
      "text/plain": [
       "                                                        geometry\n",
       "id                                                              \n",
       "BRG_620801_02  POLYGON ((111.16331 -2.89282, 111.16957 -2.892..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = api.ClientV1(api_key=PLANET_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/geopandas/geodataframe.py:852: UserWarning: Geometry column does not contain geometry.\n",
      "  warnings.warn(\"Geometry column does not contain geometry.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><table id=\"d2fdd9d7-1ede-415d-8730-26e57a057d2a\" class=\"display nowrap\"><thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead></table>\n",
       "<script type=\"text/javascript\">\n",
       "require([\"datatables\"], function (datatables) {\n",
       "    $(document).ready(function () {        \n",
       "        var dt_args = {\"columnDefs\": [{\"width\": \"70px\", \"targets\": \"_all\"}], \"paging\": false, \"data\": [[\"BRG_620801_02\", \"POLYGON ((111.1633130010653 -2.892815023311756, 111.169574970425 -2.892815023311756, 111.169574970425 -2.88651897473922, 111.1633130010653 -2.88651897473922, 111.1633130010653 -2.892815023311756))\"]]};\n",
       "        dt_args = eval_functions(dt_args);\n",
       "        table = $('#d2fdd9d7-1ede-415d-8730-26e57a057d2a').DataTable(dt_args);\n",
       "    });\n",
       "})\n",
       "</script>\n",
       "</div>\n"
      ],
      "text/plain": [
       "                                                        geometry\n",
       "id                                                              \n",
       "BRG_620801_02  POLYGON ((111.16331 -2.89282, 111.16957 -2.892..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test data for the filter\n",
    "start_date = datetime.datetime(2009, 1, 1)\n",
    "stop_date = datetime.datetime(2020, 12, 31)\n",
    "cloud_cover_lte = 0.02\n",
    "minimum_covered_area = 90 # included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_type_score\n",
    "item_type_score = {\n",
    "    'PSScene4Band':9, \n",
    "    'PSScene3Band':7, \n",
    "    'PSOrthoTile':8,\n",
    "    'REOrthoTile':0,\n",
    "    'SkySatScene':0,\n",
    "}\n",
    "\n",
    "# season score\n",
    "months_score = {\n",
    "    1: 5, 7:0,\n",
    "    2: 5, 8:0,\n",
    "    3: 5, 9:0,\n",
    "    4: 0, 10:7,\n",
    "    5: 0, 11:10,\n",
    "    6: 0, 12:10,\n",
    "}\n",
    "\n",
    "# cloud_score\n",
    "\n",
    "def cloud_score(cloud_cover):\n",
    "    \"\"\" Define the cloud cover threshold and score\n",
    "    \n",
    "    1 = 1%\n",
    "    \n",
    "    \"\"\"\n",
    "    cloud_cover = cloud_cover*100\n",
    "    \n",
    "    if cloud_cover == 0:\n",
    "        return 10\n",
    "    elif cloud_cover <= 1 and cloud_cover > 0:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Covered area\n",
    "\n",
    "def cover_score(covered_area):\n",
    "    \"\"\"Define the cover area threshold and score\n",
    "    \"\"\"\n",
    "    covered_area = covered_area*100\n",
    "    \n",
    "    if covered_area >= 99:\n",
    "        return 10\n",
    "    \n",
    "    elif covered_area >= 95:\n",
    "        return 5\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION: 1.1 Get items for individual samples ((optional))\n",
    "### Get items and metadata using filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AOI, by selecting the first row of the samples geodataframe\n",
    "# For this example, we are going to use the first sample\n",
    "row_number = 0\n",
    "aoi_geometry = json.loads(dumps(samples_gdf.iloc[row_number].geometry))\n",
    "sample_id = samples_gdf.iloc[row_number].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "request = build_request(aoi_geometry, start_date, stop_date, cloud_cover_lte)\n",
    "items = get_items(sample_id, request, client)\n",
    "\n",
    "# Transform items into a pandas dataframe with useful columns\n",
    "metadata_df = get_dataframe(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate percentage of covered area\n",
    "\n",
    "Calculate the percentage of covered area from the sample area with the item footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutate metadata_df and add the percentage of cover area\n",
    "add_cover_area(metadata_df, samples_gdf)\n",
    "# Remove items that are under 90% of covered area\n",
    "metadata_df = metadata_df[metadata_df.cover_perc >= (minimum_covered_area/100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_items = score_items(metadata_df, item_type_score, months_score, cloud_score, cover_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_month = False\n",
    "by_every = 0\n",
    "\n",
    "if by_every:\n",
    "    selected_items = get_one_item_every_x(scored_items, every=by_every)\n",
    "\n",
    "elif by_month:\n",
    "    selected_items = get_one_item_per_month(scored_items)\n",
    "\n",
    "else:\n",
    "    selected_items = get_one_item_per_year(scored_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ((Optional)): Export thumbnails\n",
    "Create thumbnails from the selected items (dataframe) and store them into a structured folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_thumb(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION 1.2 Get items for all plots and store into a big df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection method\n",
    "The loop will search all the images between the given start-end date, and the minimum cloud coverage.<br>\n",
    "After that it will calculate the sample covered area with the image item footprint and then will remove items which are under the given threshold.<br>\n",
    "The next step is rank the items by the selected parameters <br>\n",
    "#### Temporal selection\n",
    "The user has to select the desired time span for get the images: 1 per year, 1 per month, or one every x images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If by_month is True, one image per month will be chosen, otherwise one per year.\n",
    "# By default it will process only one image per year\n",
    "\n",
    "by_month = False\n",
    "by_every = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over all plots in parellel\n",
    "Loop over all plots and get the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiprocess(index, row, srch_log_file, by_month=False, by_every=0, skip_items=None):\n",
    "    \n",
    "    aoi_geometry = json.loads(dumps(row.geometry))\n",
    "    sample_id = row.name\n",
    "    \n",
    "    if by_every:\n",
    "        pickle_df_name = os.path.join(OUT_PIKL_PATH, str(sample_id)+'_every.p')\n",
    "    elif by_month:\n",
    "        pickle_df_name = os.path.join(OUT_PIKL_PATH, str(sample_id)+'_month.p')\n",
    "    else:\n",
    "        pickle_df_name = os.path.join(OUT_PIKL_PATH, str(sample_id)+'_year.p')\n",
    "        \n",
    "    if not os.path.exists(pickle_df_name):\n",
    "        request = build_request(aoi_geometry, start_date, stop_date, cloud_cover_lte)\n",
    "\n",
    "        try:\n",
    "            print(f'Starting {sample_id}')\n",
    "            items = get_items(sample_id, request, client)\n",
    "            # Transform items into a pandas dataframe with useful columns\n",
    "            metadata_df = get_dataframe(items)\n",
    "            \n",
    "            # Skip items with errors\n",
    "            if skip_items:\n",
    "                metadata_df = metadata_df[~metadata_df.id.isin(skip_items)]\n",
    "            \n",
    "            # Mutate metadata_df and add the percentage of cover area\n",
    "            add_cover_area(metadata_df, samples_gdf)\n",
    "\n",
    "            # Remove items that are under the minimum_covered_area threshold\n",
    "            metadata_df = metadata_df[metadata_df.cover_perc >= (minimum_covered_area/100)]\n",
    "\n",
    "            # Create a score for each item\n",
    "            scored_items = score_items(metadata_df, item_type_score, months_score, cloud_score, cover_score)\n",
    "            \n",
    "            if by_every:\n",
    "                # Filter scored_items and get one item every x items\n",
    "                selected_items = get_one_item_every_x(scored_items, every=by_every)\n",
    "            \n",
    "            elif by_month:\n",
    "                # Filter scored_items and get only one per month\n",
    "                selected_items = get_one_item_per_month(scored_items)\n",
    "            else:\n",
    "                # Filter scored_items and get only one per year\n",
    "                selected_items = get_one_item_per_year(scored_items)\n",
    "            \n",
    "            # Save into a pickled file\n",
    "            selected_items.to_pickle(pickle_df_name)\n",
    "            \n",
    "            print(f'{sample_id} pickled.')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'there was an error with the sample {sample_id}, please check the log files.')\n",
    "            with open(srch_log_file, 'a') as lf:\n",
    "                lf.write(f'\"{sample_id}\":{e}\\n')\n",
    "\n",
    "    else:\n",
    "        print(f'Search for {sample_id} already saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gdf = samples_gdf[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip error items from logs\n",
    "Uncomment the next cell if you have a log file with \"no access to assets\" elements, so the process will skip them.\n",
    "\n",
    "<br> If you are using this option, please delete the previous searches pickled files from the failed samples (search failed samples with the commands in step 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_items = None\n",
    "# skip_items = get_no_access_assets_from_log('logs/order_logs_20200916_15_02.txt')\n",
    "# len(skip_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Create a log file\n",
    "    now = datetime.datetime.now()\n",
    "    formated_now = now.strftime('%Y%m%d_%H_%M')\n",
    "    srch_log_file = os.path.join(LOG_PATH, f'search_logs_{formated_now}.txt')\n",
    "    \n",
    "    # Set the number of parallel processes\n",
    "    pool = multiprocessing.Pool(4)\n",
    "    \n",
    "    for index, row in samples_gdf.iterrows():\n",
    "        pool.apply_async(run_multiprocess, args=(index, row, srch_log_file, by_month, by_every, skip_items))\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the pickled files, merge and store them in a big df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_files = glob.glob(os.path.join(OUT_PIKL_PATH,'*every.p'))\n",
    "len(pickled_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([pd.read_pickle(pkl) for pkl in pickled_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Order assets\n",
    "### Create json request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_bundles = {\n",
    "\n",
    "    # Is not possible to ask for analytic_dn in PSScene3Band, so the next option is visual\n",
    "    # for more info go to https://developers.planet.com/docs/orders/product-bundles-reference/\n",
    "    'PSScene3Band': \"analytic,visual\",\n",
    "    'PSScene4Band': \"analytic_udm2,analytic_sr,analytic\",\n",
    "    'PSOrthoTile': \"analytic_5b_udm2,analytic_5b,analytic_udm2,analytic,visual\",\n",
    "    'REOrthoTile': \"analytic,visual\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the order we need a dataframe with filtered items,\n",
    "# and a samples_gdf with sample_id and geometry to clip each item.\n",
    "\n",
    "# Build an order for each sample and store in a orders_list\n",
    "orders = []\n",
    "samples_ids = list(all_df.sample_id.unique())\n",
    "for idx, row in samples_gdf.iterrows():\n",
    "    if idx in samples_ids:\n",
    "        order = build_order_from_metadata(all_df, idx, row, products_bundles)\n",
    "        orders.append(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request order\n",
    "<font color='red'>The following lines will start the order in the planet server, once the order is placed and running, there is no way to stop it.</font>\n",
    "\n",
    "NOTE: The following loop will skip the samples that have already been downloaded, however it's based on the existing_orders request, and we are not sure how long the requests will remain in the planet server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request the existing orders and store their sample_id (name)\n",
    "current_server_orders = get_existing_orders(client)\n",
    "ordered_sample_names = [order['name'] for order in current_server_orders]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "formated_now = now.strftime('%Y%m%d_%H_%M')\n",
    "ordr_log_file = os.path.join(LOG_PATH, f'order_logs_{formated_now}.txt')\n",
    "\n",
    "orders_info = []\n",
    "for new_order in orders:\n",
    "\n",
    "    # Make sure that the sample is not already downloaded\n",
    "    sample_name = new_order['name']\n",
    "    if sample_name not in ordered_sample_names:\n",
    "        \n",
    "        try:\n",
    "            # The following line will create the order in the server\n",
    "            @backoff.on_exception(backoff.expo,planet.api.exceptions.OverQuota, max_time=360)\n",
    "            def place_order():\n",
    "                return client.create_order(new_order).get()\n",
    "            \n",
    "            order_info = place_order()\n",
    "            orders_info.append(order_info)\n",
    "            \n",
    "            order_id = order_info['id']\n",
    "            sample_name = order_info['name']\n",
    "            \n",
    "            print(f'order {order_id} with {sample_name} has been placed.')\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open(ordr_log_file, 'a') as lf:\n",
    "                print(f'there was an error with the sample {sample_name}, please check the log files.')\n",
    "                lf.write(f'Sample {sample_name}:{e}\\n')\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get status\n",
    "The following get_order_status line has to be re-runned everytime we want to know the orders statusw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pages to limit the search, every page will display 20 orders.\n",
    "get_orders_status(client, pages=1).sort_values(by=['created_on'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "current_server_orders = get_existing_orders(client, pages=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the dates in which the desired oreders were ordered.\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "formated_now = now.strftime('%Y%m%d_%H_%M')\n",
    "dw_log_file = os.path.join(LOG_PATH, f'download_logs_{formated_now}.txt')\n",
    "\n",
    "current_server_orders = get_existing_orders(client)\n",
    "\n",
    "# Select the dates in which you want to download the images\n",
    "start_date = datetime.date(2020,9,16)\n",
    "stop_date = datetime.date(2020,9,16)\n",
    "success_states = ['success', 'partial']\n",
    "\n",
    "for order in current_server_orders:\n",
    "    \n",
    "    created_on = pd.to_datetime(order['created_on']).date()\n",
    "    state = order['state']\n",
    "    \n",
    "    if state in success_states:\n",
    "\n",
    "        if created_on >= start_date and created_on <= stop_date:\n",
    "            # Create the download folder\n",
    "            download_order_path = os.path.join(DOWNLOAD_PATH, order['name'])\n",
    "            Path(download_order_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Check if there is a .zip file inside the folder\n",
    "            if not any(['.zip' in f for f in os.listdir(download_order_path)]):\n",
    "                try:\n",
    "                    sample_name = order[\"name\"]\n",
    "                    print(f'downloading {sample_name} ')\n",
    "                    callback = api.write_to_file(directory=f'{download_order_path}/', overwrite=True)\n",
    "\n",
    "                    @backoff.on_exception(backoff.expo,planet.api.exceptions.OverQuota,max_time=360)\n",
    "                    def download():\n",
    "                        client.download_order(order['id'], callback=callback)\n",
    "                    download()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f'There was an error with {sample_name}, please check download log file.')\n",
    "                    with open(dw_log_file, 'w') as lf:\n",
    "                        lf.write(f'Sample {sample_name}:{e}\\n')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Additional commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_samples = [x[0] for x in get_no_access_assets_from_log('logs/order_logs_20200915_12_41_yelena.txt')]\n",
    "len(failed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "failed_items_ids = [x[1] for x in get_no_access_assets_from_log('logs/order_logs_20200915_12_41_yelena.txt')]\n",
    "failed_items_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
