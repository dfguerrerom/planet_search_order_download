{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to use your own api keys and parameters, copy paste the `parameters.py.dist` file in the same folder and remove the `.dist` extention. You can then replace the string with your own keys. only the .dist will be pushed to the dist git rep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Search items\n",
    "### Get the samples dataframe\n",
    "\n",
    "From a geojson plots file, create a geo pandas dataframe to store the geometries and the id of each plot, it'll be used as a geometry filter and to calculate the % of area covered by the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gdf = pd.read_pickle('shp/samples.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a geoDataFrame object from a .txt file \n",
    "if os.path.isfile(FILENAME):\n",
    "    df = pd.read_csv(FILENAME, sep=' ')\n",
    "    \n",
    "    #filter only the `nb_rows` first rows\n",
    "    nb_rows = 1000#len(df)\n",
    "    filter_df  = df[df.index.isin(range(nb_rows))]\n",
    "    df = filter_df\n",
    "    \n",
    "    #create the geodataframe \n",
    "    pts = [Point(df.loc[i][FILE_LNG], df.loc[i][FILE_LAT]) for i in range(len(df))]\n",
    "    samples_gdf = gpd.GeoDataFrame(data={'geometry': pts}, index=df[FILE_ID], crs=\"EPSG:4326\")\n",
    "    samples_gdf.index.names = ['id']\n",
    "    samples_gdf = samples_gdf.to_crs('ESRI:54032')\n",
    "    samples_gdf['geometry'] = samples_gdf['geometry'].buffer(BUFFER_SIZE)\n",
    "    samples_gdf = samples_gdf.to_crs(\"EPSG:4326\")\n",
    "    samples_gdf['geometry'] = samples_gdf['geometry'].envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = api.ClientV1(api_key=PLANET_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test data for the filter\n",
    "start_date = datetime.datetime(2009, 1, 1)\n",
    "stop_date = datetime.datetime(2020, 12, 31)\n",
    "cloud_cover_lte = 0.01\n",
    "minimum_covered_area = 90 # included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_type_score\n",
    "item_type_score = {\n",
    "    'PSScene4Band':8, \n",
    "    'PSScene3Band':8, \n",
    "    'PSOrthoTile':10,\n",
    "    'REOrthoTile':0,\n",
    "    'SkySatScene':0,\n",
    "}\n",
    "\n",
    "# season score\n",
    "months_score = {\n",
    "    1: 5, 7:0,\n",
    "    2: 5, 8:0,\n",
    "    3: 5, 9:0,\n",
    "    4: 0, 10:7,\n",
    "    5: 0, 11:10,\n",
    "    6: 0, 12:10,\n",
    "}\n",
    "\n",
    "# cloud_score\n",
    "\n",
    "def cloud_score(cloud_cover):\n",
    "    \"\"\" Define the cloud cover threshold and score\n",
    "    \n",
    "    1 = 1%\n",
    "    \n",
    "    \"\"\"\n",
    "    cloud_cover = cloud_cover*100\n",
    "    \n",
    "    if cloud_cover == 0:\n",
    "        return 10\n",
    "    elif cloud_cover <= 1 and cloud_cover > 0:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Covered area\n",
    "\n",
    "def cover_score(covered_area):\n",
    "    \"\"\"Define the cover area threshold and score\n",
    "    \"\"\"\n",
    "    covered_area = covered_area*100\n",
    "    \n",
    "    if covered_area >= 99:\n",
    "        return 10\n",
    "    \n",
    "    elif covered_area >= 95:\n",
    "        return 5\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION: 1.1 Get items for individual samples ((optional))\n",
    "### Get items and metadata using filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AOI, by selecting the first row of the samples geodataframe\n",
    "# For this example, we are going to use the first sample\n",
    "row_number = 2\n",
    "aoi_geometry = json.loads(dumps(samples_gdf.iloc[row_number].geometry))\n",
    "sample_id = samples_gdf.iloc[row_number].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "request = build_request(aoi_geometry, start_date, stop_date, cloud_cover_lte)\n",
    "items, response = get_items(sample_id, request, client)\n",
    "\n",
    "# Transform items into a pandas dataframe with useful columns\n",
    "metadata_df = get_dataframe(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.quick_search(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(result.items_iter(limit=1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate percentage of covered area\n",
    "\n",
    "Calculate the percentage of covered area from the sample area with the item footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutate metadata_df and add the percentage of cover area\n",
    "add_cover_area(metadata_df, samples_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove items that are under 90% of covered area\n",
    "metadata_df = metadata_df[metadata_df.cover_perc >= (minimum_covered_area/100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_items = score_items(metadata_df, item_type_score, months_score, cloud_score, cover_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_items = get_one_item_per_year(scored_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ((Optional)): Export thumbnails\n",
    "Create thumbnails from the selected items (dataframe) and store them into a structured folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_thumb(selected_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION 1.2 Get items for all plots and store into a big df\n",
    "### Loop over all plots\n",
    "Loop over all plots and get the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gdf=samples_gdf[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dataframes \n",
    "\n",
    "LOG_PATH = os.path.join(os.getcwd(), 'logs')\n",
    "Path(LOG_PATH).mkdir(parents=True, exist_ok=True)\n",
    "srch_log_file = os.path.join(LOG_PATH, 'search_logs.txt')\n",
    "\n",
    "OUT_PIKL_PATH = os.path.join(os.getcwd(), 'searches')\n",
    "Path(OUT_PIKL_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_multiprocess(index, row):    \n",
    "    aoi_geometry = json.loads(dumps(row.geometry))\n",
    "    sample_id = row.name\n",
    "    \n",
    "    pickle_df_name = os.path.join(OUT_PIKL_PATH, str(sample_id)+'.p')\n",
    "\n",
    "    if not os.path.exists(pickle_df_name):\n",
    "        request = build_request(aoi_geometry, start_date, stop_date, cloud_cover_lte)\n",
    "\n",
    "        try:\n",
    "            print(f'Starting {sample_id}')\n",
    "            items, response = get_items(sample_id, request, client)\n",
    "            # Transform items into a pandas dataframe with useful columns\n",
    "            metadata_df = get_dataframe(items)\n",
    "            \n",
    "            # Mutate metadata_df and add the percentage of cover area\n",
    "            add_cover_area(metadata_df, samples_gdf)\n",
    "\n",
    "            # Remove items that are under the minimum_covered_area threshold\n",
    "            metadata_df = metadata_df[metadata_df.cover_perc >= (minimum_covered_area/100)]\n",
    "\n",
    "            # Create a score for each item\n",
    "            scored_items = score_items(metadata_df, item_type_score, months_score, cloud_score, cover_score)\n",
    "\n",
    "            # Filter scored_items and get only one per year\n",
    "            selected_items = get_one_item_per_year(scored_items)\n",
    "            \n",
    "            # Save into a pickled file\n",
    "            selected_items.to_pickle(pickle_df_name)\n",
    "            \n",
    "            print(f'{sample_id} pickled.')\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open(srch_log_file, 'w') as lf:\n",
    "                lf.write('\\n'.join(f'{sample_id}:{e}'))\n",
    "\n",
    "    else:\n",
    "        print(f'Search for {sample_id} already saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(8)\n",
    "    for index, row in samples_gdf.iterrows():\n",
    "        pool.apply_async(run_multiprocess, args=(index, row,))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_list = glob.glob(os.path.join(OUT_PIKL_PATH, '*p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([pd.read_pickle(pkl) for pkl in pkl_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle('congo_100_200.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_pickle('congo_100_200.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df[all_df.sample_id==359827]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Order assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the order we need a dataframe with filtered items,\n",
    "# and a samples_gdf with sample_id and geometry to clip each item.\n",
    "\n",
    "# Build an order for each sample and store in a orders_list\n",
    "orders = []\n",
    "for idx, row in samples_gdf.iterrows():\n",
    "    order = build_order_from_metadata(all_df, samples_gdf, sample_id=idx)\n",
    "    orders.append(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request order\n",
    "<font color='red'>The following lines will start the order in the planet server, once the order is placed and running, there is no way to stop it.</font>\n",
    "\n",
    "NOTE: The following loop will skip the samples that have already been downloaded, however it's based on the existing_orders request, and we are not sure how long the requests will remain in the planet server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request the existing orders and store their sample_id (name)\n",
    "existing_orders = client.get_orders().get()\n",
    "ordered_sample_ids = [o['name'] for o in existing_orders['orders']]\n",
    "orders_info = []\n",
    "for new_order in orders:\n",
    "\n",
    "    # Make sure that the sample is not already downloaded\n",
    "    if new_order['name'] not in ordered_sample_ids:\n",
    "        \n",
    "        # The following line will create the order in the server\n",
    "        @backoff.on_exception(backoff.expo,\n",
    "                              (planet.api.exceptions.OverQuota, \n",
    "                              planet.api.exceptions.BadQuery),\n",
    "                              max_time=360)\n",
    "        order_info = client.create_order(new_order).get()\n",
    "        order_id = order_info['id']\n",
    "        sample_name = order_info['name']\n",
    "        orders_info.append(order_info)\n",
    "        print(f'order {order_id} with {sample_name} has been placed.')\n",
    "        sleep(2)\n",
    "    else:\n",
    "        sample_name = new_order['name']\n",
    "        print(f'Skipping {sample_name}: already requested.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# order_id = order_info['id']\n",
    "# order_id\n",
    "# track_order(order_id, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = os.path.join(os.getcwd(),'downloads')\n",
    "\n",
    "# Search all the requested orders per page\n",
    "# Fixed api.models NEXT_KEY parameter from \"_next\" to \"next\"\n",
    "\n",
    "ordered_orders = client.get_orders()\n",
    "ordered_orders.NEXT_KEY = \"next\"\n",
    "order_pages=[]\n",
    "\n",
    "# We can limit the search to certain number of pages\n",
    "# if we leave as none, will search over all of them\n",
    "limit_to_x_pages = None\n",
    "for page in ordered_orders.iter(limit_to_x_pages):\n",
    "    page.NEXT_KEY = \"next\"\n",
    "    order_pages.append(page.get())\n",
    "\n",
    "current_server_orders = [order for page in order_pages for order in page['orders']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = os.path.join(os.getcwd(), 'logs')\n",
    "Path(LOG_PATH).mkdir(parents=True, exist_ok=True)\n",
    "dw_log_file = os.path.join(LOG_PATH, 'download_logs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the dates in which the desired oreders were ordered.\n",
    "start_date = datetime.date(2020,9,10)\n",
    "stop_date = datetime.date(2020,9,10)\n",
    "success_states = ['success', 'partial']\n",
    "\n",
    "for order in current_server_orders:\n",
    "    \n",
    "    created_on = pd.to_datetime(order['created_on']).date()\n",
    "    state = order['state']\n",
    "    \n",
    "    if state in success_states:\n",
    "\n",
    "        if created_on >= start_date and created_on <= stop_date:\n",
    "            # Create the download folder\n",
    "            download_order_path = os.path.join(download_path, order['name'])\n",
    "            Path(download_order_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Check if folder is empty:\n",
    "            if not os.listdir(download_order_path) :\n",
    "                try:\n",
    "                    print(f'downloading {order[\"name\"]} ')\n",
    "                    callback = api.write_to_file(directory=f'{download_order_path}/', overwrite=True)\n",
    "\n",
    "                    @backoff.on_exception(backoff.expo,planet.api.exceptions.OverQuota,max_time=360)\n",
    "                    def download():\n",
    "                        client.download_order(order['id'], callback=callback)\n",
    "                    download()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    with open(dw_log_file, 'w') as lf:\n",
    "                        lf.write('\\n'.join(f'{sample_id}:{e}'))\n",
    "            else:\n",
    "                print(f'The folder {download_order_path} is not empty ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -H \"Authorization: api-key 7779c34186f04031b0734e1b7b7ad820\" \\\n",
    "    'https://api.planet.com/auth/v1/experimental/public/my/subscriptions'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
