{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import itables.interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_orders(client, pages=None):\n",
    "    # Search all the requested orders per page\n",
    "    # Fixed api.models NEXT_KEY parameter from \"_next\" to \"next\"\n",
    "\n",
    "    ordered_orders = client.get_orders()\n",
    "    ordered_orders.NEXT_KEY = \"next\"\n",
    "    order_pages=[]\n",
    "\n",
    "    # We can limit the search to certain number of pages\n",
    "    # if we leave as none, will search over all of them\n",
    "    limit_to_x_pages = pages\n",
    "    for page in ordered_orders.iter(limit_to_x_pages):\n",
    "        page.NEXT_KEY = \"next\"\n",
    "        order_pages.append(page.get())\n",
    "\n",
    "    current_server_orders = [order for page in order_pages for order in page['orders']]\n",
    "    \n",
    "    return current_server_orders\n",
    "\n",
    "\n",
    "\n",
    "def get_orders_status(client, pages=None):\n",
    "    current_server_orders = get_existing_orders(client, pages)\n",
    "    progress_df = pd.DataFrame([(f['created_on'], \n",
    "                                 f['last_message'], \n",
    "                                 f['last_modified'], \n",
    "                                 f['id'], \n",
    "                                 f['name'], \n",
    "                                 f['state'], ) for f in current_server_orders])\n",
    "    progress_df.columns =['created_on', 'last_message', 'last_modified', 'id', 'name', 'state',]\n",
    "    progress_df.sort_values(by=['created_on'])\n",
    "    \n",
    "    return progress_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to use your own api keys and parameters, copy paste the `parameters.py.dist` file in the same folder and remove the `.dist` extention. You can then replace the string with your own keys. only the .dist will be pushed to the dist git rep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Search items\n",
    "### Get the samples dataframe\n",
    "\n",
    "From a geojson plots file, create a geo pandas dataframe to store the geometries and the id of each plot, it'll be used as a geometry filter and to calculate the % of area covered by the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gdf = pd.read_pickle('shp/samples.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a geoDataFrame object from a .txt file \n",
    "if os.path.isfile(FILENAME):\n",
    "    df = pd.read_csv(FILENAME, sep=' ')\n",
    "    \n",
    "    #filter only the `nb_rows` first rows\n",
    "    nb_rows = 1000#len(df)\n",
    "    filter_df  = df[df.index.isin(range(nb_rows))]\n",
    "    df = filter_df\n",
    "    \n",
    "    #create the geodataframe \n",
    "    pts = [Point(df.loc[i][FILE_LNG], df.loc[i][FILE_LAT]) for i in range(len(df))]\n",
    "    samples_gdf = gpd.GeoDataFrame(data={'geometry': pts}, index=df[FILE_ID], crs=\"EPSG:4326\")\n",
    "    samples_gdf.index.names = ['id']\n",
    "    samples_gdf = samples_gdf.to_crs('ESRI:54032')\n",
    "    samples_gdf['geometry'] = samples_gdf['geometry'].buffer(BUFFER_SIZE)\n",
    "    samples_gdf = samples_gdf.to_crs(\"EPSG:4326\")\n",
    "    samples_gdf['geometry'] = samples_gdf['geometry'].envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gdf = samples_gdf[400:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gdf = samples_gdf[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = api.ClientV1(api_key=PLANET_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test data for the filter\n",
    "start_date = datetime.datetime(2009, 1, 1)\n",
    "stop_date = datetime.datetime(2020, 12, 31)\n",
    "cloud_cover_lte = 0.01\n",
    "minimum_covered_area = 90 # included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_type_score\n",
    "item_type_score = {\n",
    "    'PSScene4Band':8, \n",
    "    'PSScene3Band':8, \n",
    "    'PSOrthoTile':10,\n",
    "    'REOrthoTile':0,\n",
    "    'SkySatScene':0,\n",
    "}\n",
    "\n",
    "# season score\n",
    "months_score = {\n",
    "    1: 5, 7:0,\n",
    "    2: 5, 8:0,\n",
    "    3: 5, 9:0,\n",
    "    4: 0, 10:7,\n",
    "    5: 0, 11:10,\n",
    "    6: 0, 12:10,\n",
    "}\n",
    "\n",
    "# cloud_score\n",
    "\n",
    "def cloud_score(cloud_cover):\n",
    "    \"\"\" Define the cloud cover threshold and score\n",
    "    \n",
    "    1 = 1%\n",
    "    \n",
    "    \"\"\"\n",
    "    cloud_cover = cloud_cover*100\n",
    "    \n",
    "    if cloud_cover == 0:\n",
    "        return 10\n",
    "    elif cloud_cover <= 1 and cloud_cover > 0:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Covered area\n",
    "\n",
    "def cover_score(covered_area):\n",
    "    \"\"\"Define the cover area threshold and score\n",
    "    \"\"\"\n",
    "    covered_area = covered_area*100\n",
    "    \n",
    "    if covered_area >= 99:\n",
    "        return 10\n",
    "    \n",
    "    elif covered_area >= 95:\n",
    "        return 5\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION: 1.1 Get items for individual samples ((optional))\n",
    "### Get items and metadata using filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AOI, by selecting the first row of the samples geodataframe\n",
    "# For this example, we are going to use the first sample\n",
    "row_number = 2\n",
    "aoi_geometry = json.loads(dumps(samples_gdf.iloc[row_number].geometry))\n",
    "sample_id = samples_gdf.iloc[row_number].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "request = build_request(aoi_geometry, start_date, stop_date, cloud_cover_lte)\n",
    "items = get_items(sample_id, request, client)\n",
    "\n",
    "# Transform items into a pandas dataframe with useful columns\n",
    "metadata_df = get_dataframe(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate percentage of covered area\n",
    "\n",
    "Calculate the percentage of covered area from the sample area with the item footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutate metadata_df and add the percentage of cover area\n",
    "add_cover_area(metadata_df, samples_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove items that are under 90% of covered area\n",
    "metadata_df = metadata_df[metadata_df.cover_perc >= (minimum_covered_area/100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_items = score_items(metadata_df, item_type_score, months_score, cloud_score, cover_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_items = get_one_item_per_year(scored_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ((Optional)): Export thumbnails\n",
    "Create thumbnails from the selected items (dataframe) and store them into a structured folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_thumb(selected_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION 1.2 Get items for all plots and store into a big df\n",
    "### Loop over all plots\n",
    "Loop over all plots and get the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dataframes \n",
    "\n",
    "LOG_PATH = os.path.join(os.getcwd(), 'logs')\n",
    "Path(LOG_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_multiprocess(index, row):    \n",
    "    aoi_geometry = json.loads(dumps(row.geometry))\n",
    "    sample_id = row.name\n",
    "    \n",
    "    now = datetime.now()\n",
    "    formated_now = now.strftime('%Y%m%d_%H_%M')\n",
    "    srch_log_file = os.path.join(LOG_PATH, f'search_logs_{formated_now}.txt')\n",
    "    \n",
    "    OUT_PIKL_PATH = os.path.join(os.getcwd(), 'searches', formated_now)\n",
    "    Path(OUT_PIKL_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "    pickle_df_name = os.path.join(OUT_PIKL_PATH, str(sample_id)+'.p')\n",
    "\n",
    "    if not os.path.exists(pickle_df_name):\n",
    "        request = build_request(aoi_geometry, start_date, stop_date, cloud_cover_lte)\n",
    "\n",
    "        try:\n",
    "            print(f'Starting {sample_id}')\n",
    "            items = get_items(sample_id, request, client)\n",
    "            # Transform items into a pandas dataframe with useful columns\n",
    "            metadata_df = get_dataframe(items)\n",
    "            \n",
    "            # Mutate metadata_df and add the percentage of cover area\n",
    "            add_cover_area(metadata_df, samples_gdf)\n",
    "\n",
    "            # Remove items that are under the minimum_covered_area threshold\n",
    "            metadata_df = metadata_df[metadata_df.cover_perc >= (minimum_covered_area/100)]\n",
    "\n",
    "            # Create a score for each item\n",
    "            scored_items = score_items(metadata_df, item_type_score, months_score, cloud_score, cover_score)\n",
    "\n",
    "            # Filter scored_items and get only one per year\n",
    "            selected_items = get_one_item_per_year(scored_items)\n",
    "            \n",
    "            # Save into a pickled file\n",
    "            selected_items.to_pickle(pickle_df_name)\n",
    "            \n",
    "            print(f'{sample_id} pickled.')\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open(srch_log_file, 'a') as lf:\n",
    "                \n",
    "                lf.write(f'Sample {sample_id}:{e}\\n')\n",
    "\n",
    "    else:\n",
    "        print(f'Search for {sample_id} already saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(10)\n",
    "    for index, row in samples_gdf.iterrows():\n",
    "        pool.apply_async(run_multiprocess, args=(index, row,))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the pickled files, merge and store them in a big df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_list = glob.glob(os.path.join(OUT_PIKL_PATH, '*p'))\n",
    "all_df = pd.concat([pd.read_pickle(pkl) for pkl in pkl_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Order assets\n",
    "### Create json request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_bundles = {\n",
    "\n",
    "    # Is not possible to ask for analytic_dn in PSScene3Band, so the next option is visual\n",
    "    # for more info go to https://developers.planet.com/docs/orders/product-bundles-reference/\n",
    "    'PSScene3Band': \"analytic,visual\",\n",
    "    'PSScene4Band': \"analytic_udm2,analytic_sr,analytic\",\n",
    "    'PSOrthoTile': \"analytic_5b_udm2,analytic_5b,analytic_udm2,analytic\",\n",
    "    'REOrthoTile': \"analytic\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the order we need a dataframe with filtered items,\n",
    "# and a samples_gdf with sample_id and geometry to clip each item.\n",
    "\n",
    "# Build an order for each sample and store in a orders_list\n",
    "orders = []\n",
    "samples_ids = list(all_df.sample_id.unique())\n",
    "for idx, row in samples_gdf.iterrows():\n",
    "    if idx in samples_ids:\n",
    "        order = build_order_from_metadata(all_df, idx, row, products_bundles)\n",
    "        orders.append(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request order\n",
    "<font color='red'>The following lines will start the order in the planet server, once the order is placed and running, there is no way to stop it.</font>\n",
    "\n",
    "NOTE: The following loop will skip the samples that have already been downloaded, however it's based on the existing_orders request, and we are not sure how long the requests will remain in the planet server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request the existing orders and store their sample_id (name)\n",
    "current_server_orders = get_existing_orders(client)\n",
    "ordered_sample_names = [order['name'] for order in current_server_orders]\n",
    "now = datetime.now()\n",
    "formated_now = now.strftime('%Y%m%d_%H_%M')\n",
    "ordr_log_file = os.path.join(LOG_PATH, f'order_logs_{formated_now}.txt')\n",
    "\n",
    "orders_info = []\n",
    "for new_order in orders:\n",
    "\n",
    "    # Make sure that the sample is not already downloaded\n",
    "    sample_name = new_order['name']\n",
    "    if sample_name not in ordered_sample_names:\n",
    "        \n",
    "        try:\n",
    "            # The following line will create the order in the server\n",
    "            @backoff.on_exception(backoff.expo,planet.api.exceptions.OverQuota, max_time=360)\n",
    "            def place_order():\n",
    "                return client.create_order(new_order).get()\n",
    "            \n",
    "            order_info = place_order()\n",
    "            orders_info.append(order_info)\n",
    "            \n",
    "            order_id = order_info['id']\n",
    "            sample_name = order_info['name']\n",
    "            \n",
    "            print(f'order {order_id} with {sample_name} has been placed.')\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open(ordr_log_file, 'a') as lf:\n",
    "                \n",
    "                lf.write(f'Sample {sample_name}:{e}\\n')\n",
    "    else:\n",
    "        print(f'Skipping {sample_name}: already requested.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get status\n",
    "The following get_order_status line has to be re-runned everytime we want to know the orders statusw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pages to limit the search, every page will display 20 orders.\n",
    "get_orders_status(client, pages=None).sort_values(by=['created_on'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = os.path.join(os.getcwd(),'downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = os.path.join(os.getcwd(), 'logs')\n",
    "Path(LOG_PATH).mkdir(parents=True, exist_ok=True)\n",
    "dw_log_file = os.path.join(LOG_PATH, 'download_logs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the dates in which the desired oreders were ordered.\n",
    "\n",
    "current_server_orders = get_existing_orders(client)\n",
    "\n",
    "start_date = datetime.date(2020,9,10)\n",
    "stop_date = datetime.date(2020,9,10)\n",
    "success_states = ['success', 'partial']\n",
    "\n",
    "for order in current_server_orders:\n",
    "    \n",
    "    created_on = pd.to_datetime(order['created_on']).date()\n",
    "    state = order['state']\n",
    "    \n",
    "    if state in success_states:\n",
    "\n",
    "        if created_on >= start_date and created_on <= stop_date:\n",
    "            # Create the download folder\n",
    "            download_order_path = os.path.join(download_path, order['name'])\n",
    "            Path(download_order_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Check if folder is empty:\n",
    "            if not os.listdir(download_order_path) :\n",
    "                try:\n",
    "                    print(f'downloading {order[\"name\"]} ')\n",
    "                    callback = api.write_to_file(directory=f'{download_order_path}/', overwrite=True)\n",
    "\n",
    "                    @backoff.on_exception(backoff.expo,planet.api.exceptions.OverQuota,max_time=360)\n",
    "                    def download():\n",
    "                        client.download_order(order['id'], callback=callback)\n",
    "                    download()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    with open(dw_log_file, 'w') as lf:\n",
    "                        lf.write('\\n'.join(f'{sample_id}:{e}'))\n",
    "            else:\n",
    "                print(f'The folder {download_order_path} is not empty ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
